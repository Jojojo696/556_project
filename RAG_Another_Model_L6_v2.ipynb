{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG Demo with OLMo and ArXiv `astro-ph` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAG based approach below is a demonstration of how to use [OLMo-1B](https://huggingface.co/allenai/OLMo-1B) LLM model by AI2 to generate an abstract completion for a given input text. The input text is a random starting abstract from `astro-ph` category of [ArXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv). The abstract completion is generated by the model using the RAG approach. The RAG approach retrieves relevant documents from [Qdrant Vector Database](https://qdrant.tech/), which provides contextual information to the model for generating the completion.\n",
    "\n",
    "The input text was retrieved from the [AstroLLaMa Paper](https://arxiv.org/abs/2309.06126). Rather than fine-tuning a model, we wanted to see if RAG approach can also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following statement as user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"The Magellanic Stream (MS) - an enormous ribbon of gas spanning 140âˆ˜ of the southern\n",
    "sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since\n",
    "its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been\n",
    "conclusively identified. This stellar stream would reveal the distance and 6D kinematics of\n",
    "the MS, constraining its formation and the past orbital history of the Clouds. We\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "import fsspec\n",
    "\n",
    "def fetch_arxiv_dataset(zip_url: str) -> pd.DataFrame:\n",
    "    cols = ['id', 'title', 'abstract', 'categories']\n",
    "\n",
    "    with fsspec.open(zip_url) as f:\n",
    "        with zipfile.ZipFile(f) as archive:\n",
    "            data = []\n",
    "            json_file = archive.filelist[0]\n",
    "            with archive.open(json_file) as f:\n",
    "                for line in io.TextIOWrapper(f, encoding=\"latin-1\"):\n",
    "                    doc = json.loads(line)\n",
    "                    lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "                    data.append(lst)\n",
    "                    \n",
    "            df_data = pd.DataFrame(data=data, columns=cols)\n",
    "    return df_data\n",
    "\n",
    "# https://github.com/allenai/open-instruct/blob/main/eval/templates.py\n",
    "def create_prompt_with_olmo_chat_format(messages, bos=\"|||IP_ADDRESS|||\", eos=\"|||IP_ADDRESS|||\", add_bos=True):\n",
    "    formatted_text = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_text += \"<|system|>\\n\" + message[\"content\"] + \"\\n\"\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_text += \"<|user|>\\n\" + message[\"content\"] + \"\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            formatted_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + eos + \"\\n\"\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Olmo chat template only supports 'system', 'user' and 'assistant' roles. Invalid role: {}.\".format(message[\"role\"])\n",
    "                )\n",
    "    formatted_text += \"<|assistant|>\\n\"\n",
    "    formatted_text = bos + formatted_text  # forcibly add bos\n",
    "    return formatted_text\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents (arXiv `astro-ph` abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section retrieves the arXiv abstracts and creates documents\n",
    "for loading into a vector database. You can skip running the following sections\n",
    "if you have a local copy of the Qdrant Vector Database data ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following zip_url will be changed weekly.\n",
    "\n",
    "In case you run the code for the first time or need the latest url,  please go to website of ArXiv Dataset to download the latest dataset.\n",
    "\n",
    "#### You can find zip_url of that by clicking the info_directory-> more_info-> where_from of dataset locally.\n",
    "\n",
    "In case you have downloaded the dataset locally, you can use file path directly (e.g. file_path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_url = \"https://storage.googleapis.com/kaggle-data-sets/612177/7925852/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com@kaggle-161607.iam.gserviceaccount.com/20240327/auto/storage/goog4_request&X-Goog-Date=20240327T183523Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4747ce35edc693785c00b4ade2fc7f62149173bf160f1b04f97fc6a752bfb1ccb5408359a16b475e7d955f04a52f2fb9f916d8090330993839fabfb1835847e0c62452243ecc74e232eeed1d747beaf6da1209b9614d305c020e6bd09bb096e6c6e2bb4711d96fb457ed1533c04bb78690253d3b6f4a4068aa3b9cd073742a3ed68562fa2a88a29e646a629dee0a26f99ff0539b5f81c926bc2b5a62642ac9f0a92febc7ca812a61351191334baad93b3ecca2ac408da8ca35a4d6e8afda67d6e8196b50c20ee18358a19cb21c25dfbcc7394bc99b280ed9222c8a933ea91f7d4b65aba05156ab985b36e761a70a35f6bbd208b9507a04ff68e15c258ec5920f\"\n",
    "\n",
    "# zip_url = \"https://storage.googleapis.com/kaggle-data-sets/612177/8112112/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240420%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240420T072044Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4205122187fe955292316d8c21c161d1a5cda0cad078505415764e112ed5502117eac4d4c492908387a31c00f84a0b5ff9591b0128fbff051a6f0f675604d95325162cf0c957033fcbdfe9f070945da28c00cfbce1b7f387804228f150ab3cb1489a9e7ce1c58f5ab8b3fe3cc5f9680d19366969b43e8e0905444416f1a77314582c3335eaee1d06e8859fb95631a5baa6a75212702383dfc628ed8cc0b34b9f9a1433cca752789d20af013022f828ef2d54ea653f03649bbf45c2b1139611e3621b5844c3f9a489a79724dee8b883f22bab92c9c08e96915561a98b3862e4e93bff86eaa613fd81d88798230fe31eee129e9dda76a2aaacb565700e41b7524f\"\n",
    "\n",
    "file_path = \"/Users/amy/Desktop/archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of astro-ph papers:  1000\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset containing all arXiv abstracts\n",
    "df_data = fetch_arxiv_dataset(file_path)\n",
    "# Filter the dataset to only include astro-ph category\n",
    "astro_df = df_data[df_data.categories.str.contains('astro-ph')].reset_index(drop=True)\n",
    "astro_df=astro_df[:1000]\n",
    "print(\"Number of astro-ph papers: \", len(astro_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eargerly load the dataframe full of abstracts\n",
    "# to memory in the form of langchain Document objects\n",
    "loader = DataFrameLoader(astro_df, page_content_column=\"abstract\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_path=\"./local_qdrant_first_1000_L6_v2\"\n",
    "qdrant_collection=\"arxiv_astro-ph_abstracts_first_1000_L6_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amy/Library/Python/3.8/lib/python/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/amy/Library/Python/3.8/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Qdrant collection 'arxiv_astro-ph_abstracts_first_1000_L6_v2' from 1000 documents\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(qdrant_path):\n",
    "    print(f\"Loading existing Qdrant collection '{qdrant_collection}'\")\n",
    "    from qdrant_client import QdrantClient\n",
    "    # If the Qdrant Vector Database Collection already exists, load it\n",
    "    client = QdrantClient(path=qdrant_path)\n",
    "    qdrant = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=qdrant_collection,\n",
    "        embeddings=embedding\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new Qdrant collection '{qdrant_collection}' from {len(documents)} documents\")\n",
    "    \n",
    "    # Load the documents into a Qdrant Vector Database Collection\n",
    "    # this will save locally in the current directory as sqlite\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents,\n",
    "        embedding,\n",
    "        path=qdrant_path,\n",
    "        collection_name=qdrant_collection,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out the Qdrant collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the retriever for later step\n",
    "retriever = qdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amy/Library/Python/3.8/lib/python/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Test out the statement retrieval\n",
    "found_docs = retriever.get_relevant_documents(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  We have analyzed the HI aperture synthesis image of the Large Magellanic\n",
      "Cloud (LMC), using an objective and quantitative measure of topology to\n",
      "understand the HI distribution hosting a number of holes and clumps of various\n",
      "sizes in the medium. The HI distribution shows different topology at four\n",
      "different chosen scales. At the smallest scales explored (19-29 pc), the HI\n",
      "mass is distributed in such a way that numerous clumps are embedded on top of a\n",
      "low density background. At the larger scales from 73 to 194 pc, it shows a\n",
      "generic hole topology. These holes might have been formed mainly by stellar\n",
      "winds from hot stars. At the scales from 240 to 340 pc, slightly above the disk\n",
      "scale-height of the gaseous disk, major clumps in the HI map change the\n",
      "distribution to have a slight clump topology. These clumps include the giant\n",
      "cloud associations in the spiral arms and the thick filaments surrounding\n",
      "superholes. At the largest scales studied (390-485 pc), the hole topology is\n",
      "present again. Responsible to the hole topology at this scale are a few\n",
      "superholes which seem mainly associated with supernova explosions in the outer\n",
      "disk. The gaps between the bar and spiral arms have a minor effect on the\n",
      "topology at this scale.\n",
      "\n",
      "\n",
      "  We present ground-based B and R-band color-magnitude diagrams (CMDs),\n",
      "reaching the oldest main-sequence turnoffs with good photometric accuracy for\n",
      "twelve fields in the Small Magellanic Cloud (SMC). Our fields, located between\n",
      "~1 and ~4 degrees from the center of the galaxy, are situated in different\n",
      "parts of the SMC such as the \"Wing'' area, and towards the West and South. In\n",
      "this paper we perform a first analysis of the stellar content in our SMC fields\n",
      "through comparison with theoretical isochrones and color functions (CFs). We\n",
      "find that the underlying spheroidally distributed population is composed of\n",
      "both intermediate-age and old stars and that its age composition does not show\n",
      "strong galacto-centric gradients. The three fields situated toward the east, in\n",
      "the Wing region, show very active current star formation. However, only in the\n",
      "eastern field closest to the center do we find an enhancement of recent star\n",
      "formation with respect to a constant SFR(t). The fields corresponding to the\n",
      "western side of the SMC present a much less populated young MS, and the CF\n",
      "analysis indicates that the SFR(t) greatly diminished around 2 Gyr ago in these\n",
      "parts. Field smc0057, the closest to the center of the galaxy and located in\n",
      "the southern part, shows recent star formation, while the rest of the southern\n",
      "fields present few bright MS stars. The structure of the red clump in all the\n",
      "CMDs is consistent with the large amount of intermediate-age stars inferred\n",
      "from the CMDs and color functions. None of the SMC fields presented here are\n",
      "dominated by old stellar populations, a fact that is in agreement with the lack\n",
      "of a conspicuous horizontal branch in all these SMC CMDs. This could indicate\n",
      "that a disk population is ruling over a possible old halo in all the observed\n",
      "fields.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(format_docs(found_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in case to build olmo model correctly, we need to run the following statement to downgrade version of transformer:\n",
    "\n",
    "\n",
    "#### %pip install transformers==4.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", torch_dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What range of wavelengths is suggested as appropriate to look for dynamic signatures in the solar chromosphere according to the computations by Carlsson and Stein?\n",
      "\n",
      "\n",
      "Answer: 1.5-8\n",
      "mHz\n",
      "\n",
      "Explanation:\n",
      "  The very nature of the solar chromosphere, its structuring and dynamics,\n",
      "remains far from being properly understood, in spite of intensive research.\n",
      "Here we point out the potential of chromospheric observations at millimeter\n",
      "wavelengths to resolve this long-standing problem. Computations carried out\n",
      "with a sophisticated dynamic model of the solar chromosphere due to Carlsson\n",
      "and Stein demonstrate that millimeter emission is extremely sensitive to\n",
      "dynamic processes in the chromosphere and the appropriate wavelengths to look\n",
      "for dynamic signatures are in the range 0.8-5.0 mm. The model also suggests\n",
      "that high resolution observations at mm wavelengths, as will be provided by\n",
      "ALMA, will have the unique property of reacting to both the hot and the cool\n",
      "gas, and thus will have the potential of distinguishing between rival models of\n",
      "the solar atmosphere. Thus, initial results obtained from the observations of\n",
      "the quiet Sun at 3.5 mm with the BIMA array (resolution of 12 arcsec) reveal\n",
      "significant oscillations with amplitudes of 50-150 K and frequencies of 1.5-8\n",
      "mHz with a tendency toward short-period oscillations in internetwork and longer\n",
      "periods in network regions. However higher spatial resolution, such as that\n",
      "provided by ALMA, is required for a clean separation between the features\n",
      "within the solar atmosphere and for an adequate comparison with the output of\n",
      "the comprehensive dynamic simulations.\n",
      "   The physical interpretation of spectropolarimetric observations of lines of\n",
      "neutral helium, such as those of the 10830 A multiplet, represents an excellent\n",
      "opportunity for investigating the magnetism of plasma structures in the solar\n",
      "chromosphere. Here we present a powerful forward modeling and inversion code\n",
      "that permits either to calculate the emergent intensity and polarization for\n",
      "any given magnetic field vector or to infer the dynamical and magnetic\n",
      "properties from the observed Stokes profiles. This diagnostic tool is based on\n",
      "the quantum theory of spectral line polarization, which self-consistently\n",
      "accounts for the Hanle and Zeeman effects in the most general case of the\n",
      "incomplete Paschen-Back effect regime. We also take into account radiative\n",
      "transfer effects. An efficient numerical scheme based on global optimization\n",
      "methods has been applied. Our Stokes inversion code permits a fast and reliable\n",
      "determination of the global minimum.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What range of wavelengths is suggested as appropriate to look for dynamic signatures in the solar chromosphere according to the computations by Carlsson and Stein?\"\n",
    "\n",
    "print (f'Question: {question}\\n')\n",
    "context_docs = retriever.invoke(question)\n",
    "context = \" \".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "qa_response = qa_model(question = question, context = context)\n",
    "\n",
    "print(f'\\nAnswer: {qa_response[\"answer\"]}\\n')\n",
    "\n",
    "print(\"Explanation:\\n\"+context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation by locally running LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What range of wavelengths is suggested as appropriate to look for dynamic signatures in the solar chromosphere according to the computations by Carlsson and Stein?\"\n",
    "\n",
    "\n",
    "# context_docs = retriever.invoke(question)\n",
    "# context = \" \".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "# # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a helpful question answering chatbot, that tries to answer the question given the context.\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": f'Question: {question}, context: {context}\\n'},\n",
    "# ]\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "# print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
