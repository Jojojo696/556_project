{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Question-Answering Demo with distilbert and ArXiv `astro-ph` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "import fsspec\n",
    "\n",
    "def fetch_arxiv_dataset(zip_url: str) -> pd.DataFrame:\n",
    "    cols = ['id', 'title', 'abstract', 'categories']\n",
    "\n",
    "    with fsspec.open(zip_url) as f:\n",
    "        with zipfile.ZipFile(f) as archive:\n",
    "            data = []\n",
    "            json_file = archive.filelist[0]\n",
    "            with archive.open(json_file) as f:\n",
    "                for line in io.TextIOWrapper(f, encoding=\"latin-1\"):\n",
    "                    doc = json.loads(line)\n",
    "                    lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "                    data.append(lst)\n",
    "                    \n",
    "            df_data = pd.DataFrame(data=data, columns=cols)\n",
    "    return df_data\n",
    "\n",
    "# https://github.com/allenai/open-instruct/blob/main/eval/templates.py\n",
    "def create_prompt_with_olmo_chat_format(messages, bos=\"|||IP_ADDRESS|||\", eos=\"|||IP_ADDRESS|||\", add_bos=True):\n",
    "    formatted_text = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_text += \"<|system|>\\n\" + message[\"content\"] + \"\\n\"\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_text += \"<|user|>\\n\" + message[\"content\"] + \"\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            formatted_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + eos + \"\\n\"\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Olmo chat template only supports 'system', 'user' and 'assistant' roles. Invalid role: {}.\".format(message[\"role\"])\n",
    "                )\n",
    "    formatted_text += \"<|assistant|>\\n\"\n",
    "    formatted_text = bos + formatted_text  # forcibly add bos\n",
    "    return formatted_text\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents (arXiv `astro-ph` abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section retrieves the arXiv abstracts and creates documents\n",
    "for loading into a vector database. You can skip running the following sections\n",
    "if you have a local copy of the Qdrant Vector Database data ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following zip_url will be changed weekly.\n",
    "\n",
    "In case you run the code for the first time or need the latest url,  please go to website of ArXiv Dataset to download the latest dataset.\n",
    "\n",
    "#### You can find zip_url of that by clicking the info_directory-> more_info-> where_from of dataset locally.\n",
    "\n",
    "In case you have downloaded the dataset locally, you can use file path directly (e.g. file_path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_url = \"https://storage.googleapis.com/kaggle-data-sets/612177/8112112/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240420%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240420T072044Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4205122187fe955292316d8c21c161d1a5cda0cad078505415764e112ed5502117eac4d4c492908387a31c00f84a0b5ff9591b0128fbff051a6f0f675604d95325162cf0c957033fcbdfe9f070945da28c00cfbce1b7f387804228f150ab3cb1489a9e7ce1c58f5ab8b3fe3cc5f9680d19366969b43e8e0905444416f1a77314582c3335eaee1d06e8859fb95631a5baa6a75212702383dfc628ed8cc0b34b9f9a1433cca752789d20af013022f828ef2d54ea653f03649bbf45c2b1139611e3621b5844c3f9a489a79724dee8b883f22bab92c9c08e96915561a98b3862e4e93bff86eaa613fd81d88798230fe31eee129e9dda76a2aaacb565700e41b7524f\"\n",
    "\n",
    "file_path = \"/Users/amy/Desktop/archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of astro-ph papers:  1000\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset containing all arXiv abstracts\n",
    "df_data = fetch_arxiv_dataset(file_path)\n",
    "# Filter the dataset to only include astro-ph category\n",
    "astro_df = df_data[df_data.categories.str.contains('astro-ph')].reset_index(drop=True)\n",
    "astro_df=astro_df[:1000]\n",
    "print(\"Number of astro-ph papers: \", len(astro_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eargerly load the dataframe full of abstracts\n",
    "# to memory in the form of langchain Document objects\n",
    "loader = DataFrameLoader(astro_df, page_content_column=\"abstract\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_path=\"./local_qdrant_first_1000_L6_v2\"\n",
    "qdrant_collection=\"arxiv_astro-ph_abstracts_first_1000_L6_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amy/Library/Python/3.8/lib/python/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/amy/Library/Python/3.8/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Qdrant collection 'arxiv_astro-ph_abstracts_first_1000_L6_v2'\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(qdrant_path):\n",
    "    print(f\"Loading existing Qdrant collection '{qdrant_collection}'\")\n",
    "    from qdrant_client import QdrantClient\n",
    "    # If the Qdrant Vector Database Collection already exists, load it\n",
    "    client = QdrantClient(path=qdrant_path)\n",
    "    qdrant = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=qdrant_collection,\n",
    "        embeddings=embedding\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new Qdrant collection '{qdrant_collection}' from {len(documents)} documents\")\n",
    "    \n",
    "    # Load the documents into a Qdrant Vector Database Collection\n",
    "    # this will save locally in the current directory as sqlite\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents,\n",
    "        embedding,\n",
    "        path=qdrant_path,\n",
    "        collection_name=qdrant_collection,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the retriever for later step\n",
    "retriever = qdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", torch_dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What pattern in Faraday rotation measures (RMs) requires the presence of at least one large-scale magnetic reversal in the fourth Galactic quadrant?\n",
      "\n",
      "\n",
      "Answer: counter-clockwise\n",
      "\n",
      "Explanation:\n",
      "  We present new Faraday rotation measures (RMs) for 148 extragalactic radio\n",
      "sources behind the southern Galactic plane (253o < l < 356o, |b| < 1.5o), and\n",
      "use these data in combination with published data to probe the large-scale\n",
      "structure of the Milky Way's magnetic field. We show that the magnitudes of\n",
      "these RMs oscillate with longitude in a manner that correlates with the\n",
      "locations of the Galactic spiral arms. The observed pattern in RMs requries the\n",
      "presence of at least one large-scale magnetic reversal in the fourth Galactic\n",
      "quadrant, located between the Sagittarius- Carina and Scutum-Crux spiral arms.\n",
      "To quantitatively compare our measurements to other recent studies, we consider\n",
      "all available extragalactic and pulsar RMs in the region we have surveyed, and\n",
      "jointly fit these data to simple models in which the large-scale field follows\n",
      "the spiral arms. In the best-fitting model, the magnetic field in the fourth\n",
      "Galactic quadrant is directed clockwise in the Sagittarius-Carina spiral arm\n",
      "(as viewed from the North Galactic pole), but is oriented counter- clockwise in\n",
      "the Scutum-Crux arm. This contrasts with recent analyses of pulsar RMs alone,\n",
      "in which the fourth-quadrant field was presumed to be directed\n",
      "counter-clockwise in the Sagittarius- Carina arm. Also in contrast to recent\n",
      "pulsar RM studies, our joint modeling of pulsar and extragalactic RMs\n",
      "demonstrates that large numbers of large-scale magnetic field reversals are not\n",
      "required to account for observations.\n",
      "   Magnetorotational instability (MRI) is one of the most important and most\n",
      "common instabilities in astrophysics, it is widely accepted that it serves as a\n",
      "source of turbulent viscosity in accretion disks -- the most energy efficient\n",
      "objects in the Universe. However it is very difficult to bring this process\n",
      "down on earth and model it in a laboratory experiment. Several different\n",
      "approaches have been proposed, one of the most recent is PROMISE\n",
      "(Potsdam-ROssendorf Magnetorotational InStability Experiment). It consists of a\n",
      "flow of a liquid metal between two rotating cylinders under applied\n",
      "current-free spiral magnetic field. The cylinders must be covered with plates\n",
      "which introduce additional end-effects which alter the flow and make it more\n",
      "difficult to clearly distinguish between MRI stable and unstable state. In this\n",
      "paper we propose simple and inexpensive improvement to the PROMISE experiment\n",
      "which would reduce those undesirable effects.\n",
      "\n",
      "0.9677081108093262\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_run_time(func):\n",
    "    time_start = time.time()\n",
    "    func()\n",
    "    time_end = time.time()\n",
    "    print(time_end - time_start)\n",
    "    \n",
    "def question_answering_func(question):\n",
    "    print (f'Question: {question}\\n')\n",
    "    context_docs = retriever.invoke(question)\n",
    "    context = \" \".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "    qa_response = qa_model(question = question, context = context)\n",
    "\n",
    "    print(f'\\nAnswer: {qa_response[\"answer\"]}\\n')\n",
    "\n",
    "    print(\"Explanation:\\n\"+context)\n",
    "    \n",
    "questions = [\n",
    "    \"\",\n",
    "    \"What millimeter wavelength range is suggested as appropriate to look for dynamic signatures in the solar chromosphere according to the computations by Carlsson and Stein?\",\n",
    "    \"How does the globular cluster mass function (GCMF) in the Milky Way vary with cluster half-mass density (rho_h)?\",\n",
    "    \"What is the nature of the huge far-infrared luminosity of the Cloverleaf lensed QSO (H1413+117)?\",\n",
    "    \"What is the behavior of the angular momentum Lz​ for nearly horizon-skimming orbits around a nearly extremal Kerr black hole, and how does this behavior compare to normal black hole orbits?\",\n",
    "    \"What is the spatial relationship between the protostars and T-Tauri members in the IC 348 star cluster?\",\n",
    "    \"What are the main observational findings regarding the X-ray and radio emissions from the Galactic non-thermal radio source G328.4+0.2?\",\n",
    "    \"What are the unique advantages of radio astrometry in exoplanet discovery compared to other methods like radial velocity searches, coronagraphy, or optical interferometry?\",\n",
    "    \"What is the determined contribution of the donor star in the H waveband in the spectrum of A0620-00?\",\n",
    "    \"Which family of Jupiter Trojans in the L4 swarm is dominated by C- and P-type asteroids?\",\n",
    "    \"What pattern in Faraday rotation measures (RMs) requires the presence of at least one large-scale magnetic reversal in the fourth Galactic quadrant?\"\n",
    "]\n",
    "\n",
    "question = questions[10]\n",
    "get_run_time(lambda: question_answering_func(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
